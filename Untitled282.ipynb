{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636d2615-96ba-4b00-b2e5-ba6aece00725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HYBRID] Iteration 1/6\n",
      "[HYBRID] Iteration 2/6\n",
      "[HYBRID] Iteration 3/6\n",
      "[HYBRID] Iteration 4/6\n",
      "[HYBRID] Iteration 5/6\n",
      "[HYBRID] Iteration 6/6\n",
      "\n",
      "BEST PARAMETERS FOUND:\n",
      "{'units1': 40, 'units2': 58, 'dropout': 0.19664505929179008, 'lr': 0.004707355258697286}\n",
      "Epoch 1/20\n",
      "32583/32583 [==============================] - 84s 3ms/step - loss: 0.8316 - accuracy: 0.7730 - val_loss: 0.8326 - val_accuracy: 0.7720\n",
      "Epoch 2/20\n",
      "32583/32583 [==============================] - 84s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8326 - val_accuracy: 0.7720\n",
      "Epoch 3/20\n",
      "32583/32583 [==============================] - 93s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8328 - val_accuracy: 0.7720\n",
      "Epoch 4/20\n",
      "32583/32583 [==============================] - 84s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8327 - val_accuracy: 0.7720\n",
      "Epoch 5/20\n",
      "32583/32583 [==============================] - 84s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8323 - val_accuracy: 0.7720\n",
      "Epoch 6/20\n",
      "32583/32583 [==============================] - 89s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8328 - val_accuracy: 0.7720\n",
      "Epoch 7/20\n",
      "32583/32583 [==============================] - 90s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8324 - val_accuracy: 0.7720\n",
      "Epoch 8/20\n",
      "32583/32583 [==============================] - 70s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8324 - val_accuracy: 0.7720\n",
      "Epoch 9/20\n",
      "32583/32583 [==============================] - 69s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8330 - val_accuracy: 0.7720\n",
      "Epoch 10/20\n",
      "32583/32583 [==============================] - 75s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8325 - val_accuracy: 0.7720\n",
      "Epoch 11/20\n",
      "32583/32583 [==============================] - 84s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8325 - val_accuracy: 0.7720\n",
      "Epoch 12/20\n",
      "32583/32583 [==============================] - 85s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8324 - val_accuracy: 0.7720\n",
      "Epoch 13/20\n",
      "32583/32583 [==============================] - 83s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8327 - val_accuracy: 0.7720\n",
      "Epoch 14/20\n",
      "32583/32583 [==============================] - 82s 3ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8324 - val_accuracy: 0.7720\n",
      "Epoch 15/20\n",
      "32583/32583 [==============================] - 74s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8326 - val_accuracy: 0.7720\n",
      "Epoch 16/20\n",
      "32583/32583 [==============================] - 70s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8324 - val_accuracy: 0.7720\n",
      "Epoch 17/20\n",
      "32583/32583 [==============================] - 71s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8326 - val_accuracy: 0.7720\n",
      "Epoch 18/20\n",
      "32583/32583 [==============================] - 72s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8324 - val_accuracy: 0.7720\n",
      "Epoch 19/20\n",
      "32583/32583 [==============================] - 71s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8325 - val_accuracy: 0.7720\n",
      "Epoch 20/20\n",
      "32583/32583 [==============================] - 71s 2ms/step - loss: 0.8289 - accuracy: 0.7730 - val_loss: 0.8324 - val_accuracy: 0.7720\n",
      "10182/10182 [==============================] - 15s 1ms/step\n",
      "\n",
      "Hybrid AIS+PSO Accuracy: 0.7732933933251899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Method `model.to_yaml()` has been removed due to security risk of arbitrary code execution. Please use `model.to_json()` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 206\u001b[39m\n\u001b[32m    199\u001b[39m     json.dump({\n\u001b[32m    200\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(acc),\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbest_params\u001b[39m\u001b[33m\"\u001b[39m: best_particle,\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(label_encoder.classes_)\n\u001b[32m    203\u001b[39m     }, f, indent=\u001b[32m4\u001b[39m)\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(BASE, \u001b[33m\"\u001b[39m\u001b[33mhybrid_fog_model.yaml\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     f.write(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# GRAPHS\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m    211\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m,\u001b[32m5\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3405\u001b[39m, in \u001b[36mModel.to_yaml\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   3382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_yaml\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m   3383\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns a yaml string containing the network configuration.\u001b[39;00m\n\u001b[32m   3384\u001b[39m \n\u001b[32m   3385\u001b[39m \u001b[33;03m    Note: Since TF 2.6, this method is no longer supported and will raise a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3403\u001b[39m \u001b[33;03m        RuntimeError: announces that the method poses a security risk\u001b[39;00m\n\u001b[32m   3404\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   3406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMethod `model.to_yaml()` has been removed due to security risk of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33marbitrary code execution. Please use `model.to_json()` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3408\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Method `model.to_yaml()` has been removed due to security risk of arbitrary code execution. Please use `model.to_json()` instead."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle, json, yaml\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ================================================================\n",
    "# PATHS\n",
    "# ================================================================\n",
    "BASE = r\"C:\\Users\\NXTWAVE\\Downloads\\Fog Density & Visibility Prediction System\"\n",
    "\n",
    "# Load CSVs\n",
    "hum = pd.read_csv(os.path.join(BASE, r\"archive (1)\\humidity.csv\"))\n",
    "pres = pd.read_csv(os.path.join(BASE, r\"archive (1)\\pressure.csv\"))\n",
    "temp = pd.read_csv(os.path.join(BASE, r\"archive (1)\\temperature.csv\"))\n",
    "weat = pd.read_csv(os.path.join(BASE, r\"archive (1)\\weather_description.csv\"))\n",
    "wdir = pd.read_csv(os.path.join(BASE, r\"archive (1)\\wind_direction.csv\"))\n",
    "wspd = pd.read_csv(os.path.join(BASE, r\"archive (1)\\wind_speed.csv\"))\n",
    "city = pd.read_csv(os.path.join(BASE, r\"archive (1)\\city_attributes.csv\"))\n",
    "\n",
    "city.rename(columns={\"City\": \"city\"}, inplace=True)\n",
    "\n",
    "# Melt\n",
    "def melt_df(df, var):\n",
    "    return df.melt(id_vars=[\"datetime\"], var_name=\"city\", value_name=var)\n",
    "\n",
    "hum = melt_df(hum, \"humidity\")\n",
    "pres = melt_df(pres, \"pressure\")\n",
    "temp = melt_df(temp, \"temperature\")\n",
    "weat = melt_df(weat, \"weather_description\")\n",
    "wdir = melt_df(wdir, \"wind_direction\")\n",
    "wspd = melt_df(wspd, \"wind_speed\")\n",
    "\n",
    "# Merge all\n",
    "df = hum.merge(pres, on=[\"datetime\", \"city\"])\n",
    "df = df.merge(temp, on=[\"datetime\", \"city\"])\n",
    "df = df.merge(weat, on=[\"datetime\", \"city\"])\n",
    "df = df.merge(wdir, on=[\"datetime\", \"city\"])\n",
    "df = df.merge(wspd, on=[\"datetime\", \"city\"])\n",
    "df = df.merge(city, on=\"city\")\n",
    "\n",
    "# Label creation\n",
    "fog_map = {\n",
    "    \"fog\": \"fog\", \"mist\": \"mist\", \"haze\": \"haze\", \"smoke\": \"smoke\",\n",
    "    \"rain\": \"rain\", \"snow\": \"snow\", \"thunderstorm\": \"storm\"\n",
    "}\n",
    "\n",
    "def categorize(desc):\n",
    "    d = str(desc).lower()\n",
    "    for k, v in fog_map.items():\n",
    "        if k in d:\n",
    "            return v\n",
    "    return \"clear\"\n",
    "\n",
    "df[\"fog_label\"] = df[\"weather_description\"].apply(categorize)\n",
    "\n",
    "# Features\n",
    "X = df[[\"humidity\", \"pressure\", \"temperature\", \"wind_direction\", \"wind_speed\"]]\n",
    "y = df[\"fog_label\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "with open(os.path.join(BASE, \"hybrid_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_enc, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# MODEL CREATION\n",
    "# ================================================================\n",
    "def build_model(units1, units2, dropout, lr):\n",
    "    dropout = max(0.05, min(dropout, 0.5))        # clip dropout\n",
    "    lr = max(0.0005, min(lr, 0.02))               # clip LR\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(int(units1), activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(dropout),\n",
    "        Dense(int(units2), activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(len(np.unique(y_enc)), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(lr),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ================================================================\n",
    "# HYBRID AIS + PSO\n",
    "# ================================================================\n",
    "num_particles = 6\n",
    "max_iters = 6\n",
    "\n",
    "particles = []\n",
    "velocities = []\n",
    "\n",
    "for _ in range(num_particles):\n",
    "    p = {\n",
    "        \"units1\": random.randint(32, 128),\n",
    "        \"units2\": random.randint(16, 64),\n",
    "        \"dropout\": random.uniform(0.1, 0.4),\n",
    "        \"lr\": random.uniform(0.001, 0.01)\n",
    "    }\n",
    "    particles.append(p)\n",
    "    velocities.append({k: 0 for k in p})\n",
    "\n",
    "best_particle = None\n",
    "best_score = -1\n",
    "\n",
    "# AIS Mutation\n",
    "def mutate(p):\n",
    "    p = p.copy()\n",
    "    if random.random() < 0.5:\n",
    "        p[\"units1\"] = random.randint(32, 128)\n",
    "    if random.random() < 0.5:\n",
    "        p[\"units2\"] = random.randint(16, 64)\n",
    "    if random.random() < 0.5:\n",
    "        p[\"dropout\"] = random.uniform(0.1, 0.4)\n",
    "    if random.random() < 0.5:\n",
    "        p[\"lr\"] = random.uniform(0.001, 0.01)\n",
    "    return p\n",
    "\n",
    "# Evaluate\n",
    "def evaluate(p):\n",
    "    p[\"units1\"] = int(max(16, min(p[\"units1\"], 256)))\n",
    "    p[\"units2\"] = int(max(8, min(p[\"units2\"], 128)))\n",
    "    p[\"dropout\"] = float(max(0.05, min(p[\"dropout\"], 0.5)))\n",
    "    p[\"lr\"] = float(max(0.0005, min(p[\"lr\"], 0.02)))\n",
    "\n",
    "    model = build_model(p[\"units1\"], p[\"units2\"], p[\"dropout\"], p[\"lr\"])\n",
    "    hist = model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "    return hist.history[\"accuracy\"][-1]\n",
    "\n",
    "# MAIN LOOP\n",
    "for it in range(max_iters):\n",
    "    print(f\"[HYBRID] Iteration {it+1}/{max_iters}\")\n",
    "\n",
    "    for i, p in enumerate(particles):\n",
    "        mutated = mutate(p)\n",
    "        score = evaluate(mutated)\n",
    "\n",
    "        # Update global best\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_particle = mutated.copy()\n",
    "\n",
    "        # PSO update\n",
    "        w = 0.5\n",
    "        c1 = c2 = 1.5\n",
    "\n",
    "        for key in p:\n",
    "            r1, r2 = random.random(), random.random()\n",
    "            velocities[i][key] = (\n",
    "                w * velocities[i][key]\n",
    "                + c1 * r1 * (best_particle[key] - p[key])\n",
    "                + c2 * r2 * (mutated[key] - p[key])\n",
    "            )\n",
    "            p[key] += velocities[i][key]\n",
    "\n",
    "print(\"\\nBEST PARAMETERS FOUND:\")\n",
    "print(best_particle)\n",
    "\n",
    "# ================================================================\n",
    "# TRAIN FINAL MODEL\n",
    "# ================================================================\n",
    "model = build_model(**best_particle)\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nHybrid AIS+PSO Accuracy:\", acc)\n",
    "\n",
    "# ================================================================\n",
    "# SAVE MODELS & METRICS\n",
    "# ================================================================\n",
    "model.save(os.path.join(BASE, \"hybrid_fog_model.h5\"))\n",
    "\n",
    "with open(os.path.join(BASE, \"hybrid_fog_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"weights\": model.get_weights(), \"label_encoder\": label_encoder}, f)\n",
    "\n",
    "with open(os.path.join(BASE, \"hybrid_fog_model.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"accuracy\": float(acc),\n",
    "        \"best_params\": best_particle,\n",
    "        \"labels\": list(label_encoder.classes_)\n",
    "    }, f, indent=4)\n",
    "\n",
    "with open(os.path.join(BASE, \"hybrid_fog_model.yaml\"), \"w\") as f:\n",
    "    f.write(model.to_yaml())\n",
    "\n",
    "# ================================================================\n",
    "# GRAPHS\n",
    "# ================================================================\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "plt.title(\"Hybrid AIS+PSO Accuracy\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(BASE, \"hybrid_fog_accuracy_curve.png\"))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.title(\"Hybrid AIS+PSO Loss\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(BASE, \"hybrid_fog_loss_curve.png\"))\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Hybrid Confusion Matrix\")\n",
    "plt.savefig(os.path.join(BASE, \"hybrid_fog_confusion_matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df[[\"humidity\",\"pressure\",\"temperature\",\"wind_direction\",\"wind_speed\"]].corr(),\n",
    "            annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Hybrid Correlation Heatmap\")\n",
    "plt.savefig(os.path.join(BASE, \"hybrid_fog_correlation_heatmap.png\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ‰ ALL HYBRID AIS + PSO FILES & GRAPHS GENERATED SUCCESSFULLY!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc68eae-82f1-4aa9-b94e-9b431c9367de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
